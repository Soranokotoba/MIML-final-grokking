model_type: mlp
transformer: 
    d_model: 128
    n_head: 4
    num_layers: 2
mlp:
    num_layers: 3
    embed_dim: 128
    hidden_dim: [512, 256, 128]
lstm:
    num_layers: 2
    embed_dim: 128
    hidden_dim: 128
optim:
    type: AdamW
    lr: 
        type: const
        init_lr: 1e-3
        warm_up: True
        warmup_steps: 10
    beta1: 0.9
    beta2: 0.98
train:
    p: 97
    alpha: 0.5
    batch_size: 512
    epochs: 100000