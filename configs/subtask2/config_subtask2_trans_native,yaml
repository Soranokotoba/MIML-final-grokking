model_type: transformer_native # ['transformer', 'transformer_native', 'mlp', 'lstm']
transformer: 
    d_model: 128
    n_head: 4
    num_layers: 2
mlp:
    num_layers: 3
    embed_dim: 128
    hidden_dim: [512, 256, 128]
lstm:
    num_layers: 2
    embed_dim: 128
    hidden_dim: 128
optim:
    type: AdamW
    lr: 
        type: const # ['const']
        init_lr: 1e-3
        warm_up: True
        warmup_steps: 10 # used only when warm_up is True
    beta1: 0.9
    beta2: 0.98
train:
    p: 97
    alpha: 0.5
    batch_size: 512
    epochs: 10000