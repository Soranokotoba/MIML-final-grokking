model_type: transformer # ['transformer', 'transformer_native']
transformer: 
    d_model: 128
    n_head: 4
    num_layers: 2
optim:
    type: AdamW
    lr: 
        type: const # ['const']
        init_lr: 5e-4
        warm_up: True
        warmup_steps: 10 # used only when warm_up is True
    beta1: 0.9
    beta2: 0.98
train:
    p: 97
    alpha: 0.5
    batch_size: 512
    epochs: 10000
