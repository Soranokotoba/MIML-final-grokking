model_type: transformer
transformer: 
    d_model: 128
    n_head: 4
    num_layers: 2
optim:
    type: AdamW
    lr: 
        type: const
        init_lr: 1e-3
        warm_up: True
        warmup_steps: 10
    beta1: 0.9
    beta2: 0.98
train:
    p: 97
    alpha: 0.5 # [0.3, 0.4, 0.5, 0.6, 0.7]
    batch_size: 512
    epochs: 50000
  
