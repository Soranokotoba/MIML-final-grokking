model_type: transformer_native # [transformer, transformer_native]
transformer: 
    d_model: 128
    n_head: 4
    num_layers: 2
optimizer:
    type: AdamW
    lr: 1e-3
    betas: (0.9, 0.98)
train:
    p: 97
    alpha: 0.5
    batch_size: 512
    epochs: 200